# -*- coding: utf-8 -*-
"""PranavSingla_06.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NrYu6AvCG7Be3K2FdSrykTIsQvgYEPgD

Training on the student performance set
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier  # ✅ Added Gradient Boost
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             balanced_accuracy_score, confusion_matrix)
import time
import os

# Set the number of K folds as a global variable
K_FOLDS = 2

# Read the dataset from CSV file
df = pd.read_csv('/content/drive/MyDrive/Student_performance_data.csv')

# Take 20% of the data
df = df.sample(frac=0.2, random_state=42)

# Rename the last column as 'label'
df.rename(columns={df.columns[-1]: 'label'}, inplace=True)

X = df.drop(columns=['label']).values
y = df['label'].values

timing_results = []

# Define classifiers and metrics
classifiers = {
    'DecisionTree': DecisionTreeClassifier(),
    'KNN': KNeighborsClassifier(),
    'LogisticRegression': LogisticRegression(),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, subsample=0.8, random_state=42),  # ✅ Added Gradient Boost
}

# Store results
results = []

kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)

# Create a directory to save confusion matrices
os.makedirs("confusion_matrices", exist_ok=True)

# Helper function for confusion matrix metrics
def confusion_matrix_metrics(cm, classes):
    metrics = {}
    for idx, class_label in enumerate(classes):
        TP = cm[idx, idx]  # True Positives
        FP = cm[:, idx].sum() - TP  # False Positives
        FN = cm[idx, :].sum() - TP  # False Negatives
        TN = cm.sum() - (TP + FP + FN)  # True Negatives

        metrics[class_label] = {
            'TPR': TP / (TP + FN + 1e-10) if (TP + FN) > 0 else 0,
            'TNR': TN / (TN + FP + 1e-10) if (TN + FP) > 0 else 0,
            'FPR': FP / (FP + TN + 1e-10) if (FP + TN) > 0 else 0,
            'FNR': FN / (FN + TP + 1e-10) if (FN + TP) > 0 else 0
        }
    return metrics

# Iterate over classifiers
for clf_name, clf in classifiers.items():
    fold_idx = 1
    for train_index, test_index in kf.split(X):
        # Split the data
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Train the model
        start_train_time = time.time()
        clf.fit(X_train, y_train)
        train_time = time.time() - start_train_time

        # Test the model
        start_test_time = time.time()
        y_pred = clf.predict(X_test)
        test_time = time.time() - start_test_time

        timing_results.append({
            'Classifier': clf_name,
            'Fold': fold_idx,
            'Training Time (s)': train_time,
            'Testing Time (s)': test_time,
            'Total Time (s)': train_time + test_time
        })

        # Compute confusion matrix metrics
        unique_classes = np.unique(y)
        cm = confusion_matrix(y_test, y_pred, labels=unique_classes)
        cm_metrics = confusion_matrix_metrics(cm, unique_classes)

        class_metrics_list = []

        for class_label in unique_classes:
            class_mask = (y_test == class_label)
            if class_mask.sum() == 0:
                class_specific_metrics = {
                    'Classifier': clf_name,
                    'Fold': fold_idx,
                    'Class': class_label,
                    'Accuracy': np.nan,
                    'Precision': np.nan,
                    'Recall': np.nan,
                    'F1 Score': np.nan,
                    'Balanced Accuracy': np.nan,
                    'True Positive Rate (TPR)': np.nan,
                    'True Negative Rate (TNR)': np.nan,
                    'False Positive Rate (FPR)': np.nan,
                    'False Negative Rate (FNR)': np.nan,
                    'Training Time (s)': train_time,
                    'Testing Time (s)': test_time
                }
            else:
                class_specific_metrics = {
                    'Classifier': clf_name,
                    'Fold': fold_idx,
                    'Class': class_label,
                    'Accuracy': accuracy_score(y_test[class_mask], y_pred[class_mask]) if np.any(class_mask) else np.nan,
                    'Precision': precision_score(y_test[class_mask], y_pred[class_mask], average='weighted', zero_division=0) if np.any(class_mask) else np.nan,
                    'Recall': recall_score(y_test[class_mask], y_pred[class_mask], average='weighted') if np.any(class_mask) else np.nan,
                    'F1 Score': f1_score(y_test[class_mask], y_pred[class_mask], average='weighted') if np.any(class_mask) else np.nan,
                    'Balanced Accuracy': balanced_accuracy_score(y_test[class_mask], y_pred[class_mask]) if np.any(class_mask) else np.nan,
                    'True Positive Rate (TPR)': cm_metrics[class_label]['TPR'],
                    'True Negative Rate (TNR)': cm_metrics[class_label]['TNR'],
                    'False Positive Rate (FPR)': cm_metrics[class_label]['FPR'],
                    'False Negative Rate (FNR)': cm_metrics[class_label]['FNR'],
                    'Training Time (s)': train_time,
                    'Testing Time (s)': test_time
                }

            class_metrics_list.append(class_specific_metrics)

        # Plot and save confusion matrix
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)
        plt.title(f"{clf_name} - Fold {fold_idx} Confusion Matrix")
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.savefig(f"confusion_matrices/{clf_name}_fold_{fold_idx}.png")
        plt.close()

        # Append results for this fold
        results.extend(class_metrics_list)
        fold_idx += 1

# Save timing results
timing_df = pd.DataFrame(timing_results)
timing_df.to_csv("time.csv", index=False)

# Create a DataFrame for results
results_df = pd.DataFrame(results)
print("Classification Metrics Across Folds:")
print(results_df)

# Save results to CSV
results_df.to_csv("metrics.csv", index=False)

"""testing"""

import pandas as pd
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Load your data (replace 'data_path.csv' with your actual file path)
data_path = '/content/drive/MyDrive/part-00001_preprocessed_dataset.csv'  # Specify the path to your dataset
data = pd.read_csv(data_path)

# Assume your target variable is in the last column
X = data.iloc[:, :-1]  # Features (all columns except the last)
y = data.iloc[:, -1]   # Target (the last column)

# Apply PCA to reduce the features to 14
pca = PCA(n_components=14)
X_pca = pca.fit_transform(X)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)

# Train a Gradient Boosting Classifier
model = GradientBoostingClassifier()
model.fit(X_train, y_train)

# Make predictions and evaluate the model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy of the model: {accuracy * 100:.2f}%')